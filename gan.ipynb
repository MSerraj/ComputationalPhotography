{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 16\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "epochs = 100\n",
    "save_interval = 10\n",
    "scale_factor = 4  # Upscaling factor\n",
    "\n",
    "# Paths\n",
    "data_dir = \"data/DIV2K_train_HR\"\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpscaleDataset(Dataset):\n",
    "    def __init__(self, data_dir, scale_factor=4, hr_size=128):\n",
    "        self.data_dir = data_dir\n",
    "        self.scale_factor = scale_factor\n",
    "        self.hr_size = hr_size\n",
    "        self.lr_size = hr_size // scale_factor\n",
    "        \n",
    "        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
    "                           if f.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        \n",
    "        # Use Resize instead of RandomCrop to ensure all images have the same dimensions\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size, hr_size)),  # Resize to fixed dimensions\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((self.lr_size, self.lr_size)),  # Resize to fixed dimensions\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_files[idx]).convert('RGB')\n",
    "        \n",
    "        # Create HR and LR versions\n",
    "        hr_img = self.hr_transform(img)\n",
    "        lr_img = self.lr_transform(img)\n",
    "        \n",
    "        return {'lr': lr_img, 'hr': hr_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = UpscaleDataset(data_dir, scale_factor=scale_factor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Dataset size: {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "            nn.BatchNorm2d(in_features)\n",
    "        )\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.prelu(x + self.block(x))\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor, num_res_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        res_blocks = [ResidualBlock(64) for _ in range(num_res_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        # Upsampling layers\n",
    "        upsample_layers = []\n",
    "        for _ in range(int(np.log2(scale_factor))):\n",
    "            upsample_layers.extend([\n",
    "                nn.Conv2d(64, 256, 3, padding=1),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.PReLU()\n",
    "            ])\n",
    "        self.upsampling = nn.Sequential(*upsample_layers)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.conv3 = nn.Conv2d(64, 3, 9, padding=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.res_blocks(out1)\n",
    "        out3 = self.conv2(out2)\n",
    "        out4 = out1 + out3\n",
    "        out5 = self.upsampling(out4)\n",
    "        out = self.conv3(out5)\n",
    "        return torch.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 128, 128)):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = input_shape\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, stride=1, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        layers = []\n",
    "        # First layer without normalization\n",
    "        layers.extend(discriminator_block(in_channels, 64, 1, normalize=False))\n",
    "        # Add downsampling blocks\n",
    "        layers.extend(discriminator_block(64, 64, 2))\n",
    "        layers.extend(discriminator_block(64, 128, 1))\n",
    "        layers.extend(discriminator_block(128, 128, 2))\n",
    "        layers.extend(discriminator_block(128, 256, 1))\n",
    "        layers.extend(discriminator_block(256, 256, 2))\n",
    "        layers.extend(discriminator_block(256, 512, 1))\n",
    "        layers.extend(discriminator_block(512, 512, 2))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "        layers.append(nn.Conv2d(512, 1024, 1))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        layers.append(nn.Conv2d(1024, 1, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator(scale_factor=scale_factor).to(device)\n",
    "discriminator = Discriminator(input_shape=(3, dataset.hr_size, dataset.hr_size)).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# Print model summaries\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion_content = nn.L1Loss().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, dataloader, generator, discriminator, save_interval=10):\n",
    "    # Sample images for visualization\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    fixed_lr = sample_batch['lr'][:5].to(device)\n",
    "    fixed_hr = sample_batch['hr'][:5].to(device)\n",
    "    \n",
    "    # Lists to store loss values\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            epoch_g_loss = 0\n",
    "            epoch_d_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                # Configure input\n",
    "                imgs_lr = batch['lr'].to(device)\n",
    "                imgs_hr = batch['hr'].to(device)\n",
    "                batch_size = imgs_lr.size(0)\n",
    "                \n",
    "                # Adversarial ground truths\n",
    "                valid = torch.ones((batch_size, 1, 1, 1), requires_grad=False).to(device)\n",
    "                fake = torch.zeros((batch_size, 1, 1, 1), requires_grad=False).to(device)\n",
    "                \n",
    "                # ------------------\n",
    "                #  Train Generator\n",
    "                # ------------------\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                # Generate a high resolution image from low resolution input\n",
    "                gen_hr = generator(imgs_lr)\n",
    "                \n",
    "                # Adversarial loss (make discriminator think generated images are real)\n",
    "                loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
    "                \n",
    "                # Content loss (pixel-wise difference between generated and real HR images)\n",
    "                loss_content = criterion_content(gen_hr, imgs_hr)\n",
    "                \n",
    "                # Total generator loss\n",
    "                loss_G = loss_content + 1e-3 * loss_GAN\n",
    "                \n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "                \n",
    "                # -----------------------\n",
    "                #  Train Discriminator\n",
    "                # -----------------------\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Loss of real and fake images\n",
    "                loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
    "                loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                loss_D = (loss_real + loss_fake) / 2\n",
    "                \n",
    "                loss_D.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                epoch_g_loss += loss_G.item()\n",
    "                epoch_d_loss += loss_D.item()\n",
    "                pbar.set_postfix(G_loss=loss_G.item(), D_loss=loss_D.item())\n",
    "                pbar.update()\n",
    "            \n",
    "            # Record average epoch losses\n",
    "            g_losses.append(epoch_g_loss / len(dataloader))\n",
    "            d_losses.append(epoch_d_loss / len(dataloader))\n",
    "            \n",
    "            # Save model checkpoints\n",
    "            if (epoch+1) % save_interval == 0:\n",
    "                torch.save(generator.state_dict(), f\"{output_dir}/generator_epoch_{epoch+1}.pth\")\n",
    "                torch.save(discriminator.state_dict(), f\"{output_dir}/discriminator_epoch_{epoch+1}.pth\")\n",
    "            \n",
    "            # Generate and save sample images\n",
    "            with torch.no_grad():\n",
    "                gen_imgs = generator(fixed_lr)\n",
    "                # Denormalize\n",
    "                gen_imgs = (gen_imgs + 1) / 2.0\n",
    "                fixed_hr_norm = (fixed_hr + 1) / 2.0\n",
    "                fixed_lr_upscaled = torch.nn.functional.interpolate(fixed_lr, scale_factor=scale_factor, mode='nearest')\n",
    "                fixed_lr_norm = (fixed_lr_upscaled + 1) / 2.0\n",
    "                \n",
    "                # Create image grid\n",
    "                img_grid = make_grid(torch.cat([fixed_lr_norm, gen_imgs, fixed_hr_norm], -1), nrow=5)\n",
    "                save_image(img_grid, f\"{output_dir}/epoch_{epoch+1}.png\", normalize=False)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(g_losses, label=\"Generator Loss\")\n",
    "    plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{output_dir}/loss_curves.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "generator, discriminator = train(\n",
    "    epochs=epochs,\n",
    "    dataloader=dataloader,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    save_interval=save_interval\n",
    ")\n",
    "\n",
    "# Save final models\n",
    "torch.save(generator.state_dict(), f\"{output_dir}/generator_final.pth\")\n",
    "torch.save(discriminator.state_dict(), f\"{output_dir}/discriminator_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_upscale(image_path, generator, scale_factor):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Image transformations\n",
    "    lr_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Convert image to tensor\n",
    "    lr_image = lr_transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate SR image\n",
    "    with torch.no_grad():\n",
    "        sr_image = generator(lr_image)\n",
    "        sr_image = (sr_image + 1) / 2.0  # Denormalize\n",
    "    \n",
    "    # Bicubic upscaling for comparison\n",
    "    bicubic = transforms.Resize((sr_image.size(2), sr_image.size(3)), interpolation=Image.BICUBIC)\n",
    "    bicubic_img = bicubic(img)\n",
    "    bicubic_tensor = lr_transform(bicubic_img).unsqueeze(0).to(device)\n",
    "    bicubic_tensor = (bicubic_tensor + 1) / 2.0  # Denormalize\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    sr_img = sr_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    bicubic_img = bicubic_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    lr_upscaled = torch.nn.functional.interpolate(lr_image, scale_factor=scale_factor, mode='nearest')\n",
    "    lr_upscaled = (lr_upscaled + 1) / 2.0  # Denormalize\n",
    "    lr_img = lr_upscaled.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Nearest Neighbor Upscaling')\n",
    "    plt.imshow(np.clip(lr_img, 0, 1))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Bicubic Upscaling')\n",
    "    plt.imshow(np.clip(bicubic_img, 0, 1))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('GAN Upscaling')\n",
    "    plt.imshow(np.clip(sr_img, 0, 1))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/test_result.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained generator\n",
    "generator.load_state_dict(torch.load(f\"{output_dir}/generator_final.pth\"))\n",
    "generator.eval()\n",
    "\n",
    "# Test on a custom image\n",
    "test_image_path = \"data/test/resized_0818.png\"\n",
    "test_upscale(test_image_path, generator, scale_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
